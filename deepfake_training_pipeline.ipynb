{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 0) IMPORT & CONFIG\n",
    "# ==========================================\n",
    "import os, sys, platform, random, json, shutil, math, time, pathlib, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"GPU:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1408248",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 1) HYPERPARAMETERS\n",
    "# ==========================================\n",
    "IMG_SIZE = (224, 224)      # EfficientNetB0 input size\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER = 2048\n",
    "\n",
    "N_SHARDS = 15              # số đợt train nhỏ\n",
    "EPOCHS_PER_ROUND = 5       # epoch mỗi shard\n",
    "STEPS_PER_EPOCH_CAP = 300  # giới hạn steps/epoch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ec035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2) DATA ROOT & CLASS LOADING\n",
    "# ==========================================\n",
    "DATA_ROOT = \"/kaggle/input/deepfake-and-real-images/Dataset\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"Train\")\n",
    "VAL_DIR   = os.path.join(DATA_ROOT, \"Validation\")\n",
    "TEST_DIR  = os.path.join(DATA_ROOT, \"Test\")\n",
    "\n",
    "# mapping class name -> int label\n",
    "classes = sorted(os.listdir(TRAIN_DIR))\n",
    "class_indices = {cls: idx for idx, cls in enumerate(classes)}\n",
    "print(\"Classes:\", class_indices)\n",
    "\n",
    "def load_files_labels(root_dir):\n",
    "    files, labels = [], []\n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(root_dir, cls)\n",
    "        for f in glob.glob(os.path.join(cls_dir, \"*.jpg\")):\n",
    "            files.append(f)\n",
    "            labels.append(class_indices[cls])\n",
    "    return files, labels\n",
    "\n",
    "train_files, train_labels = load_files_labels(TRAIN_DIR)\n",
    "val_files,   val_labels   = load_files_labels(VAL_DIR)\n",
    "test_files,  test_labels  = load_files_labels(TEST_DIR)\n",
    "\n",
    "print(\"Train:\", len(train_files), \"Val:\", len(val_files), \"Test:\", len(test_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3) SHARD SPLITTING\n",
    "# ==========================================\n",
    "def make_shards(files, labels, n_shards):\n",
    "    files = np.array(files)\n",
    "    labels = np.array(labels)\n",
    "    idx = np.arange(len(files))\n",
    "    np.random.shuffle(idx)\n",
    "    files, labels = files[idx], labels[idx]\n",
    "    return np.array_split(files, n_shards), np.array_split(labels, n_shards)\n",
    "\n",
    "train_shards = list(zip(*make_shards(train_files, train_labels, N_SHARDS)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4) DATA PIPELINE\n",
    "# ==========================================\n",
    "def process_path(file_path, label):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)   # scale [0,1]\n",
    "    img = tf.image.resize(img, IMG_SIZE)                  # (224,224,3)\n",
    "    return img, label\n",
    "\n",
    "def make_ds(files, labels, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((files, labels))\n",
    "    ds = ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(SHUFFLE_BUFFER)\n",
    "    ds = ds.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "val_ds  = make_ds(val_files, val_labels, training=False)\n",
    "test_ds = make_ds(test_files, test_labels, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13932b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5) MODEL\n",
    "# ==========================================\n",
    "base = keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    input_shape=(224,224,3),\n",
    "    pooling=\"avg\",\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base.trainable = False\n",
    "\n",
    "x = keras.layers.Dense(128, activation=\"relu\")(base.output)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "output = keras.layers.Dense(len(classes), activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=base.input, outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")]\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 6) CALLBACKS\n",
    "# ==========================================\n",
    "ckpt_path = \"efficientnetb0_deepfake_best.keras\"\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_auc\", mode=\"max\", save_best_only=True, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=5, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbddf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 7) TRAINING WITH SHARDS\n",
    "# ==========================================\n",
    "history_all = []\n",
    "round_counter = 0\n",
    "\n",
    "for shard_idx, (files_shard, labels_shard) in enumerate(train_shards):\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\">>> ROUND {shard_idx+1}/{N_SHARDS} | shard size = {len(files_shard)}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    train_ds = make_ds(files_shard, labels_shard, training=True)\n",
    "\n",
    "    steps_per_epoch = None\n",
    "    if STEPS_PER_EPOCH_CAP is not None:\n",
    "        steps_per_epoch = min(\n",
    "            math.ceil(len(files_shard) / BATCH_SIZE),\n",
    "            STEPS_PER_EPOCH_CAP\n",
    "        )\n",
    "\n",
    "    hist = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS_PER_ROUND,\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=1,\n",
    "    )\n",
    "    history_all.append(hist.history)\n",
    "\n",
    "    round_counter += 1\n",
    "    if round_counter == max(2, N_SHARDS // 2):\n",
    "        print(\"\\n>> Unfreezing base model for fine-tuning...\")\n",
    "        base.trainable = True\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-4),\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 8) EVALUATION\n",
    "# ==========================================\n",
    "# Validation\n",
    "val_probs = model.predict(val_ds, verbose=1)\n",
    "val_pred = np.argmax(val_probs, axis=1)\n",
    "val_true = []\n",
    "for _, y in val_ds:\n",
    "    val_true.extend(y.numpy())\n",
    "\n",
    "print(\"\\nClassification report (Validation):\")\n",
    "print(classification_report(val_true, val_pred, target_names=classes, digits=4))\n",
    "\n",
    "cm = confusion_matrix(val_true, val_pred)\n",
    "df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "print(\"\\nConfusion matrix:\\n\", df_cm)\n",
    "df_cm.to_csv(\"/kaggle/working/confusion_matrix.csv\", index=True)\n",
    "print(\">> Saved confusion_matrix.csv\")\n",
    "\n",
    "# Test\n",
    "test_probs = model.predict(test_ds, verbose=1)\n",
    "test_pred = np.argmax(test_probs, axis=1)\n",
    "test_true = []\n",
    "for _, y in test_ds:\n",
    "    test_true.extend(y.numpy())\n",
    "\n",
    "print(\"\\nClassification report (Test):\")\n",
    "print(classification_report(test_true, test_pred, target_names=classes, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ff84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 9) SAVE FINAL MODEL\n",
    "# ==========================================\n",
    "model.save(\"/kaggle/working/efficientnetb0_final.keras\")\n",
    "print(\">> Saved final model.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
